{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a22d5c-cf89-492b-bb78-d9c7cc8f0006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# XOR dataset\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([0, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcf82373-1b5b-4de1-8116-e359dd532787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sine dataset with additive white Gaussian noise\n",
    "X_sine = np.linspace(0, 2*np.pi, 100).reshape(-1, 1)\n",
    "y_sine = np.sin(X_sine) + np.random.normal(0, 0.1, size=(100, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0010740-4dcd-4196-8ac4-b9124f345c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d31dbdc-1896-45ce-a768-ccc878c8f9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the single hidden layer neural network architecture\n",
    "class SingleLayerNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SingleLayerNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define the two hidden layer neural network architecture\n",
    "class TwoLayerNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
    "        super(TwoLayerNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# XOR dataset\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Sine dataset with additive white Gaussian noise\n",
    "X_sine = np.linspace(0, 2*np.pi, 100).reshape(-1, 1)\n",
    "y_sine = np.sin(X_sine) + np.random.normal(0, 0.1, size=(100, 1))\n",
    "\n",
    "# Training function\n",
    "def train_model(model, inputs, labels, criterion, optimizer, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim_xor = 2  # for XOR\n",
    "input_dim_sine = 1  # for sine dataset\n",
    "output_dim = 1\n",
    "lr = 0.1\n",
    "epochs = 1000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bccd4e4e-a830-4d8d-b08a-9c6e21a9b24b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Training single hidden layer neural network for XOR\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m hidden_dim_xor \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m51\u001b[39m)  \u001b[38;5;66;03m# random hidden layer dimension between 2 and 50\u001b[39;00m\n\u001b[0;32m      3\u001b[0m model_xor_single \u001b[38;5;241m=\u001b[39m SingleLayerNN(input_dim_xor, hidden_dim_xor, output_dim)\n\u001b[0;32m      4\u001b[0m criterion_xor \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Training single hidden layer neural network for XOR\n",
    "hidden_dim_xor = np.random.randint(2, 51)  # random hidden layer dimension between 2 and 50\n",
    "model_xor_single = SingleLayerNN(input_dim_xor, hidden_dim_xor, output_dim)\n",
    "criterion_xor = nn.BCEWithLogitsLoss()\n",
    "optimizer_xor_single = optim.SGD(model_xor_single.parameters(), lr=lr)\n",
    "train_model(model_xor_single, torch.tensor(X_xor, dtype=torch.float32), torch.tensor(y_xor, dtype=torch.float32).view(-1, 1), criterion_xor, optimizer_xor_single, epochs)\n",
    "\n",
    "# Training two hidden layer neural network for XOR\n",
    "hidden_dim1_xor = np.random.randint(2, 51)  # random hidden layer 1 dimension between 2 and 50\n",
    "hidden_dim2_xor = np.random.randint(2, 51)  # random hidden layer 2 dimension between 2 and 50\n",
    "model_xor_two = TwoLayerNN(input_dim_xor, hidden_dim1_xor, hidden_dim2_xor, output_dim)\n",
    "criterion_xor = nn.BCEWithLogitsLoss()\n",
    "optimizer_xor_two = optim.SGD(model_xor_two.parameters(), lr=lr)\n",
    "train_model(model_xor_two, torch.tensor(X_xor, dtype=torch.float32), torch.tensor(y_xor, dtype=torch.float32).view(-1, 1), criterion_xor, optimizer_xor_two, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eab2bbbe-ccf8-4156-b362-06b1705b4f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training single hidden layer neural network for sine dataset\n",
    "hidden_dim_sine = np.random.randint(2, 51)  # random hidden layer dimension between 2 and 50\n",
    "model_sine_single = SingleLayerNN(input_dim_sine, hidden_dim_sine, output_dim)\n",
    "criterion_sine = nn.MSELoss()\n",
    "optimizer_sine_single = optim.SGD(model_sine_single.parameters(), lr=lr)\n",
    "train_model(model_sine_single, torch.tensor(X_sine, dtype=torch.float32), torch.tensor(y_sine, dtype=torch.float32), criterion_sine, optimizer_sine_single, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c2fb89b-2aaf-4157-8639-80bdb488ef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training two hidden layer neural network for sine dataset\n",
    "hidden_dim1_sine = np.random.randint(2, 51)  # random hidden layer 1 dimension between 2 and 50\n",
    "hidden_dim2_sine = np.random.randint(2, 51)  # random hidden layer 2 dimension between 2 and 50\n",
    "model_sine_two = TwoLayerNN(input_dim_sine, hidden_dim1_sine, hidden_dim2_sine, output_dim)\n",
    "criterion_sine = nn.MSELoss()\n",
    "optimizer_sine_two = optim.SGD(model_sine_two.parameters(), lr=lr)\n",
    "train_model(model_sine_two, torch.tensor(X_sine, dtype=torch.float32), torch.tensor(y_sine, dtype=torch.float32), criterion_sine, optimizer_sine_two, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "930ab18e-f548-4a55-aa24-1c64deeeac30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR Test Prediction (Single Hidden Layer): 0.4927390217781067\n",
      "XOR Test Prediction (Two Hidden Layers): 0.998663067817688\n",
      "Sine Test Prediction (Single Hidden Layer): 0.8080942630767822\n",
      "Sine Test Prediction (Two Hidden Layers): 1.0215080976486206\n"
     ]
    }
   ],
   "source": [
    "# Test predictions\n",
    "with torch.no_grad():\n",
    "    test_input_xor = torch.tensor([[1, 0]], dtype=torch.float32)\n",
    "    print(\"XOR Test Prediction (Single Hidden Layer):\", torch.sigmoid(model_xor_single(test_input_xor)).item())\n",
    "    print(\"XOR Test Prediction (Two Hidden Layers):\", torch.sigmoid(model_xor_two(test_input_xor)).item())\n",
    "    \n",
    "    test_input_sine = torch.tensor([[np.pi/2]], dtype=torch.float32)\n",
    "    print(\"Sine Test Prediction (Single Hidden Layer):\", model_sine_single(test_input_sine).item())\n",
    "    print(\"Sine Test Prediction (Two Hidden Layers):\", model_sine_two(test_input_sine).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d473d1-bec6-49d2-8638-8750671d3846",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''In summary, while both the single hidden layer and two hidden layer neural networks demonstrate some degree of success in handling the \n",
    "simple test cases, the two hidden layer network emerges as the superior choice. Its increased capacity for capturing non-linear \n",
    "relationships allows it to achieve higher prediction accuracy in both the XOR problem and the sine regression task.Two hidden layer networks are \n",
    "superior due to their increased capacity to capture complex patterns.Model complexity plays a crucial role in handling \n",
    "non-linear relationships effectively.\n",
    "These results highlight the importance of model complexity in effectively modeling complex relationships in data.'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
